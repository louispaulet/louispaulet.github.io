## Benchmarking AI: Large Language Models and 3D Maze Generation with Three.js üéÆ

![Screenshot of a 3D maze generated by Three.js](./post_images/maze_benchmark.png)

[This project is live! - Check the website](https://louispaulet.github.io/maze_benchmark/)

---

### Overview

This post presents an exciting new benchmark project aimed at testing the capabilities of various Large Language Models (LLMs) by challenging them to generate interactive 3D mazes using Three.js. As AI continues to evolve, its applications in coding are expanding rapidly, and this project dives into how well LLMs can handle complex tasks like maze generation and 3D visualization in the browser.

The Maze Benchmark project serves as a testbed for evaluating how well different LLMs can translate textual instructions into functional and engaging code. In this post, I‚Äôll cover the implementation details, benchmarking methodology, and the results of each LLM's performance, as well as some exciting advanced testing with models like GPT-o1.

---

### Understanding the Maze Benchmark Project üéØ

The Maze Benchmark project pushes LLMs to create a 30x30 block maze using the recursive backtracking algorithm and render it in a 3D environment with Three.js. Key project objectives include:

- **Maze Generation**: Building the maze with a recursive backtracking algorithm.
- **3D Visualization**: Rendering the maze in a 3D scene using Three.js for a fully immersive user experience.
- **Interactivity**: Enabling mouse controls so users can explore the maze by rotating the view.
- **Aesthetics**: Differentiating walls and floors with distinct colors to enhance clarity and visual appeal.

By giving each LLM the same task, the project allows us to benchmark their ability to follow detailed coding instructions and deliver functional, interactive experiences in the browser.

---

### A Quick Look at Three.js üìò

Three.js is a popular open-source JavaScript library that simplifies the creation of 3D graphics in the browser. With Three.js, developers can build rich, interactive 3D experiences without needing to dive deep into WebGL. This makes it an ideal library for testing LLMs in tasks involving 3D rendering, like maze generation.

---

### Benchmarking Methodology üìè

To ensure consistency in testing, the following methodology was applied:

1. **Standardized Prompt**: All LLMs received the same prompt to generate a 3D maze using Three.js. The prompt included code snippets and CDNs for Three.js and OrbitControls, ensuring each model started with the same foundation.

   ```html
   Please make a simple HTML page that contains a JS code to generate a maze (30x30 blocks).  
   Use recursive backtracking algorithm.
   The user can use their mouse to rotate the scene.
   The camera should be placed above the center of the maze and show the entire maze in a top-down manner.
   Use three.js lib, here are the CDNs to use:

   import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.121.1/build/three.module.js';
   import { OrbitControls } from 'https://cdn.jsdelivr.net/npm/three@0.121.1/examples/jsm/controls/OrbitControls.js';
   ```

2. **Multiple Iterations**: Each LLM was allowed 4-5 iterations to refine their code, address bugs, and align with the task‚Äôs requirements.

3. **Enhancements**: After the initial implementation, I introduced enhancements, like improving color contrast, to improve user experience.

4. **Camera Adjustments**: Some models needed minor camera setup tweaks to ensure the maze was displayed correctly, a task that sometimes requires manual fine-tuning.

---

### Results: How the LLMs Performed üåê

Below is the ranking of each LLM based on their ability to generate the maze. Feel free to interact with the mazes by using your mouse to rotate the scene.

1. **[GPT-o1 Maze - The New Benchmark](https://louispaulet.github.io/maze_benchmark/o1_version_A.html)**  
   *GPT-o1 set a new standard, single-shot perfect maze generation.*

2. **[GPT-4o Maze](https://louispaulet.github.io/maze_benchmark/gpt4o_version.html)**  
   *An optimized version showcasing impressive performance improvements. 3 interactions.*

3. **[GPT-4 Reference Maze](https://louispaulet.github.io/maze_benchmark/gpt4_maze.html)**  
   *The initial benchmark showing GPT-4‚Äôs coding proficiency. 4-5 interactions.*

4. **[LLAMA3-70B Version](https://louispaulet.github.io/maze_benchmark/llama70b_version.html)**  
   *Required more iterations but produced a functional maze. 6 interactions.*

5. **[GPT-4o Open Interpreter Version](https://louispaulet.github.io/maze_benchmark/gpt4o_open_interpreter_version.html)**  
   *Solid effort with minor issues in interactivity.*

6. **[Zephyr-Orpo-141b-A35b-v0.1 Version](https://louispaulet.github.io/maze_benchmark/zephyr-orpo-141b-A35b-v0.1_version.html)**  
   *A promising output, though still needing refinement.*

7. **[c4ai-command-r-plus Version](https://louispaulet.github.io/maze_benchmark/c4ai-command-r-plus_version.html)**  
   *Struggled with performance and visual quality.*

8. **[Nous-Hermes-2-Mixtral-8x7B-DPO Version](https://louispaulet.github.io/maze_benchmark/Nous-Hermes-2-Mixtral-8x7B-DPO_version.html)**  
   *Encountered issues with OrbitControls and execution.*

---

### Advanced Testing with GPT-o1 üöÄ

To further push the limits of GPT-o1, I expanded the project to include more complex game development tasks:

- **Procedural Car Game**: A car game where players can navigate a procedurally generated terrain.  
   *[Play the Car Game](https://louispaulet.github.io/maze_benchmark/car_game/car.html)*

- **AGAR.IO Clone**: A multiplayer game clone focusing on real-time interaction and player growth.  
   *[Try the AGAR.IO Clone](https://louispaulet.github.io/maze_benchmark/agario/agario.html)*

- **Minecraft-Inspired Sandbox**: A simplified Minecraft environment for terrain exploration and basic building mechanics.  
   *[Explore the Minecraft Clone](https://louispaulet.github.io/maze_benchmark/minecraft/minecraft.html)*

GPT-o1 performed admirably, showcasing an advanced understanding of game mechanics and real-time interaction in these projects.

---

### Key Insights from the Benchmark üîç

The Maze Benchmark results provide valuable insights into the varying levels of proficiency among LLMs:

- **Top Performers (GPT-4, GPT-o1)**: These models consistently produced high-quality code with minimal iterations, highlighting their advanced capabilities.
  
- **Mid-Tier Models (LLAMA3-70B, Zephyr-Orpo)**: Required more iterations and guidance but still delivered functional mazes.

- **Lower-Tier Models (Nous-Hermes-2, c4ai-command-r-plus)**: Struggled with code generation and debugging, demonstrating the limitations of less advanced models.

---

### Contributing to the Maze Benchmark Project ü§ù

I encourage developers and researchers to contribute to the Maze Benchmark project. Your insights and improvements can help push this project even further. To get involved:

- **Fork the Repository**: Access the project on [GitHub](https://github.com/louispaulet/maze_benchmark).
- **Submit Pull Requests**: Implement your enhancements and submit them for review.
- **Report Issues**: Open an issue if you encounter bugs or have suggestions.

Your contributions will help advance our understanding of what LLMs can achieve in code generation.

---

### Conclusion

The Maze Benchmark project showcases the rapid progress in AI-driven code generation, particularly with models like GPT-4 and GPT-o1, which consistently demonstrate their ability to generate efficient, interactive 3D environments. While challenges remain, particularly with lower-tier models, this project highlights the exciting potential of LLMs in software development and interactive experiences.

As AI continues to evolve, it‚Äôs essential to foster collaboration, consider ethical implications, and guide its development in a responsible direction.

[This project is live! - Check the website](https://louispaulet.github.io/maze_benchmark/)